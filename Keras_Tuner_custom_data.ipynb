{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L0ogpinXDv0Q",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/media/christian/USB STICK/Images/Subset\"\n",
        "seed = 10\n",
        "image_width = 1920\n",
        "image_height = 1090\n",
        "im_shape = (image_width, image_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "        validation_split=0.2,\n",
        "        # width_shift_range=0.2,\n",
        "        # height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        # shear_range=0.2,\n",
        "        zoom_range=0.1,\n",
        "        rotation_range=20,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to_Qwva7D5Yr",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "train_generator = data_generator.flow_from_directory(DATA_DIR, target_size=im_shape, shuffle=True, seed=seed,\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
        "validation_generator = data_generator.flow_from_directory(DATA_DIR, target_size=im_shape, shuffle=False, seed=seed,\n",
        "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
        "\n",
        "\n",
        "nb_train_samples = train_generator.samples\n",
        "nb_validation_samples = validation_generator.samples\n",
        "classes = list(train_generator.class_indices.keys())\n",
        "print('Classes: '+str(classes))\n",
        "num_classes  = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "print(type(nb_train_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "counter = Counter(train_generator.classes)\n",
        "max_val = float(max(counter.values()))\n",
        "class_weights = {class_id : nb_train_samples/(num_images * num_classes) for class_id, num_images in counter.items()}\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCq5itiWHnm8"
      },
      "source": [
        "The code below defines the layers and the hyperparameters to be tuned, as well as the parameters for tuning the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IVXb_JtnEHO-",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def model_builder(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    #Learning Rate\n",
        "    hp_initial_learning_rate = hp.Choice('learning_rate',\n",
        "                                          values=[0.01, 0.001, 0.0001])\n",
        "    hp_decay_rate = hp.Float('decay_rate', min_value=0.9, max_value=0.98,\n",
        "                            step=0.01)\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      hp_initial_learning_rate,\n",
        "      decay_steps=100000,\n",
        "      decay_rate=hp_decay_rate,\n",
        "      staircase=True)\n",
        "\n",
        "    #Conv Layers\n",
        "    hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'gelu'])\n",
        "    hp_kernel_size = hp.Int('kernel_size', min_value=1, max_value=6, step=1)\n",
        "    hp_filters = hp.Int('filters', min_value=8, max_value=72, step = 8)\n",
        "    number_of_layers = hp.Int('layer_amt', min_value=1, max_value=3, step=1)\n",
        "    hp_pool_size = hp.Int('pool_size', min_value=1, max_value=4, step=1)\n",
        "\n",
        "    for layer_num in range(number_of_layers):\n",
        "      model.add(tf.keras.layers.Conv2D(filters=round(hp_filters/(layer_num+1)),\n",
        "                                      kernel_size=(hp_kernel_size, hp_kernel_size),\n",
        "                                      activation=hp_activation,\n",
        "                                      input_shape=(32, 32, 3)))\n",
        "      model.add(tf.keras.layers.MaxPool2D(pool_size=(hp_pool_size, hp_pool_size), strides=None, padding='valid'))\n",
        "      #Add maxpooling layers dumbass\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "\n",
        "    #Dense Layers\n",
        "\n",
        "    hp_layer_1 = hp.Int('layer_1', min_value=50, max_value=1000, step=50)\n",
        "    hp_layer_2 = hp.Int('layer_2', min_value=50, max_value=1000, step=50)\n",
        "\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(units=hp_layer_1, activation=hp_activation))\n",
        "    model.add(tf.keras.layers.Dense(units=hp_layer_2, activation=hp_activation))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F-uNEjnGw4d",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='dir',\n",
        "                     project_name='custom-data',\n",
        "                     overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wzR5w23GHT5X",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "#this will stop the training if the model's val_loss does not improve over 3\n",
        "#consecutive epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aewez5HH4JZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "tuner.search(train_generator, epochs=10, validation_data=validation_generator,\n",
        "             callbacks=[TensorBoard(\"/tmp/tb_logs\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0oJAQQ1YX5E"
      },
      "outputs": [],
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials=3)\n",
        "print(\"Best run:\")\n",
        "print(best_hps[0].values)\n",
        "print(\"2nd best run:\")\n",
        "print(best_hps[1].values)\n",
        "print(\"3rd best run:\")\n",
        "print(best_hps[2].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCNhICDXYftW"
      },
      "outputs": [],
      "source": [
        "model = tuner.hypermodel.build(best_hps[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgUOuLW0Yj1s"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2,\n",
        "                    callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x_X7RIyYdgb"
      },
      "source": [
        "Best model: val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uy43zJXkC-7"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir /tmp/tb_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coz0s500LQsr"
      },
      "outputs": [],
      "source": [
        "!zip -r tensorboard_logs.zip /tmp/tb_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu9Sln0YLUGl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
